{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESNET152V2\n",
    "Using pre-trained weights to improve trainin speed.\n",
    "\n",
    "Here, we use pre-trained RESNET weights to start the model.  We will only import the top of the network and add new layers at the bottom to reflect our desired output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup local Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir    = '../../data/FashionDataSet/'\n",
    "train_dir   = os.path.join(data_dir, 'train')\n",
    "images_file = os.path.join(data_dir, 'train.csv' )\n",
    "cat_file    = os.path.join(data_dir, 'label_descriptions.json' )\n",
    "os.path.isdir(data_dir), os.path.isdir(train_dir), os.path.isfile(images_file), os.path.isfile(images_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# fix libiomp5.dylib error for mac\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "# OPTIONAL SETTING\n",
    "# Here we override the keras backend env variable to use plaidml\n",
    "# plaidml can make use of AMD GPUs \n",
    "# This assignment needs to be added before loading keras libraries\n",
    "\n",
    "# os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "\n",
    "# to install plaidML, activate appropriate environment and then:\n",
    "#   pip install -U plaidml-keras\n",
    "#   plaidml-setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import datetime, os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.lines as lines\n",
    "from matplotlib.patches import Polygon\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import json\n",
    "import pandas as pd\n",
    "import skimage\n",
    "import random\n",
    "# from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "from mrcnn.visualize import display_images\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras imports\n",
    "if os.environ.get(\"KERAS_BACKEND\") == \"plaidml.keras.backend\":\n",
    "    print(\"Loading Plaid libraries for Keras.\")\n",
    "    import keras\n",
    "    \n",
    "    from keras.models import Model, load_model, model_from_json\n",
    "    from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout\n",
    "    from keras.layers.core import Lambda, RepeatVector, Reshape\n",
    "    from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "    from keras.layers.pooling import MaxPooling2D, GlobalMaxPool2D\n",
    "    from keras.layers.merge import concatenate, add\n",
    "    from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "    from keras.optimizers import Adam, SGD\n",
    "    from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "    from keras.utils import to_categorical\n",
    "    from keras.layers import UpSampling2D\n",
    "    from keras.preprocessing import image\n",
    "    from keras.applications import VGG19\n",
    "\n",
    "else:\n",
    "    print(\"Loading Tensorflow libraries for Keras.\")\n",
    "    import tensorflow as tf\n",
    "\n",
    "    from tensorflow.python.keras.models import Model, load_model, model_from_json\n",
    "    from tensorflow.python.keras.layers import Input, BatchNormalization, Activation, Dense, Dropout\n",
    "    from tensorflow.python.keras.layers.core import Lambda, RepeatVector, Reshape\n",
    "    from tensorflow.python.keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "    from tensorflow.python.keras.layers.pooling import MaxPooling2D, GlobalMaxPool2D\n",
    "    from tensorflow.python.keras.layers.merge import concatenate, add\n",
    "    from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "    from tensorflow.python.keras.optimizers import Adam, SGD\n",
    "    from tensorflow.python.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "    from tensorflow.python.keras.utils import to_categorical\n",
    "    from tensorflow.python.keras.layers import UpSampling2D\n",
    "    from tensorflow.python.keras.preprocessing import image\n",
    "    from tensorflow.python.keras.applications import VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionDataset(utils.Dataset):\n",
    "    \"\"\"\n",
    "    Implements mrcnn.utils.Dataset.\n",
    "    FashionDataset holds data relevant to the imaterialist challenge data.  \n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def load(filepath:str):\n",
    "        with open(filepath, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            return data\n",
    "        \n",
    "    def save(self, filepath:str):\n",
    "        with open(save_file, 'wb') as f:\n",
    "            pickle.dump(self, filepath)\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(FashionDataset, self).__init__()  \n",
    "        self.class_names= []\n",
    "    \n",
    "    def create_classes(self, cat_file:str) -> [dict]:\n",
    "        \"\"\"\n",
    "        Added to FashionDataset.\n",
    "        Initialize the classes.\n",
    "        param:cat_file - filepath to fashion dataset's label_descriptions.json file\n",
    "        \"\"\"\n",
    "        # read labels file\n",
    "        with open(cat_file, 'r') as data_file:\n",
    "            data=data_file.read()\n",
    "\n",
    "        # parse file\n",
    "        labels = json.loads(data)\n",
    "\n",
    "        categories = labels.get('categories')\n",
    "        df_categories = pd.DataFrame(categories)\n",
    "        df_categories['source'] = \"imaterialist\"\n",
    "\n",
    "        dict_categories = [dict(x[1]) for x in df_categories.iterrows()]\n",
    "\n",
    "        for c in dict_categories:\n",
    "            self.add_class(c['source'], c['id']+1, c['name']) # add 1 to make room for background\n",
    "\n",
    "        print (\"{} classes added.\".format(len(dict_categories)))\n",
    "\n",
    "        return dict_categories\n",
    "    \n",
    "    \n",
    "    def create_anns(self, sub_df_images:pd.DataFrame) -> dict:\n",
    "        \"\"\"\n",
    "        Creates an 'annotations' entry in an image's image_info entry.\n",
    "        dict_keys(['id', 'image_id', 'segmentation', 'category_id', 'area', 'iscrowd', 'bbox']\n",
    "        \"\"\"\n",
    "        annotations = []\n",
    "        \n",
    "        for mask in sub_df_images.iterrows():\n",
    "            h      = int(mask[1].get('height'))\n",
    "            w      = int(mask[1].get('width'))\n",
    "#             counts = [int(x) for x in mask[1]['EncodedPixels'].split(' ')]\n",
    "            counts = np.fromstring(mask[1]['EncodedPixels'], dtype=int, sep=\" \")\n",
    "            ann_dict = {'id'            : mask[1]['id'] , \n",
    "                        'image_id'      : mask[1]['file_name'] , \n",
    "                        'segmentation'  : {'counts' : counts, 'size': [h, w] }, \n",
    "                        'category_id'   : int(mask[1]['ClassId'].split('_')[0])+1, # add 1 to make room for background\n",
    "#                         'area'          : 0, \n",
    "                        'iscrowd'       : True, # True indicates the use of uncompressed RLE\n",
    "                        'bbox'          : [] }\n",
    "\n",
    "            annotations.append(ann_dict)\n",
    "            \n",
    "        return annotations\n",
    "        \n",
    "    \n",
    "    def create_images(self, images_file:str, train_dir:str, imgids:list=None, limit:int=None) -> (dict, pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Build the image_info['images'] dictionary element with all images.\n",
    "        If imgids list is None, all images in the images_file will be included, otherwise,\n",
    "        only the imgids in the list will be included.\n",
    "        \"\"\"\n",
    "        \n",
    "        df_images = pd.read_csv(images_file, nrows=limit)\n",
    "        \n",
    "        # restrict the dataframe to items in imgids list, if list is provided\n",
    "        if imgids is not None:\n",
    "            df_images = df_images[df_images.ImageId.isin(imgids)]\n",
    "        \n",
    "        df_images.rename(columns={\"ImageId\":'file_name', \"Height\":'height', \"Width\":'width'}, inplace=True)\n",
    "        df_images['id'] = [x for x in range(len(df_images))]\n",
    "        df_images['source'] = 'imaterialist'\n",
    "        \n",
    "        dict_images = [dict(x[1]) for x in tqdm(df_images.iterrows(), desc=\"Create images dict\", total=len(df_images))]\n",
    "        \n",
    "        for image in tqdm(dict_images, desc=\"Add images to object\"):\n",
    "            file_path = os.path.join(train_dir,image['file_name'])\n",
    "            self.add_image(source       = image['source'], \n",
    "                           image_id     = image['id'],\n",
    "                           path         = file_path, \n",
    "                           height       = image['height'],\n",
    "                           width        = image['width'],\n",
    "                           file_name    = image['file_name'],\n",
    "                           annotations  = self.create_anns(df_images[df_images.file_name==image['file_name']]))\n",
    "            \n",
    "        print(\"Added {} images.\".format(len(df_images)))\n",
    "\n",
    "        return self.image_info\n",
    "    \n",
    "    \n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"\n",
    "        Load the specified image and return a [H,W,3] Numpy array.\n",
    "        \"\"\"\n",
    "        # Load image\n",
    "        image = skimage.io.imread(self.image_info[image_id]['path'])\n",
    "        # If grayscale. Convert to RGB for consistency.\n",
    "        if image.ndim != 3:\n",
    "            image = skimage.color.gray2rgb(image)\n",
    "        # If has an alpha channel, remove it for consistency\n",
    "        if image.shape[-1] == 4:\n",
    "            image = image[..., :3]\n",
    "        return image\n",
    "    \n",
    "    \n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Load instance masks for the given image.\n",
    "\n",
    "        Different datasets use different ways to store masks. This\n",
    "        function converts the different mask format to one format\n",
    "        in the form of a bitmap [height, width, instances].\n",
    "\n",
    "        Returns:\n",
    "        masks: A bool array of shape [height, width, instance count] with\n",
    "            one mask per instance.\n",
    "        class_ids: a 1D array of class IDs of the instance masks.\n",
    "        \"\"\"\n",
    "        \n",
    "        image_info = self.image_info[image_id]\n",
    "\n",
    "        instance_masks = []\n",
    "        class_ids = []\n",
    "        \n",
    "        # returns list of masks/annotations for the image\n",
    "        annotations = self.image_info[image_id][\"annotations\"] \n",
    "        \n",
    "        # Build mask of shape [height, width, instance_count] and list\n",
    "        # of class IDs that correspond to each channel of the mask.\n",
    "        for annotation in annotations:\n",
    "            class_id = annotation['category_id'] # one of 46 categories\n",
    "            \n",
    "            if class_id:\n",
    "                # passes an element of the annotations list - this is a single mask entry\n",
    "#                 m = self.annToMask(annotation, image_info[\"height\"], image_info[\"width\"])\n",
    "\n",
    "                # updated to reflect problems with original maskutils implementtaion of decode\n",
    "                m = self.kaggle_rle_decode(annotation, image_info[\"height\"], image_info[\"width\"])\n",
    "                \n",
    "                # Some objects are so small that they're less than 1 pixel area\n",
    "                # and end up rounded out. Skip those objects.\n",
    "                if m.max() < 1:\n",
    "                    continue\n",
    "                # Is it a crowd? If so, use a negative class ID.\n",
    "                if annotation['iscrowd']:\n",
    "                    # Use negative class ID for crowds\n",
    "#                     class_id *= -1  # mark: not sure why this was done so commetning it out\n",
    "                    # For crowd masks, annToMask() sometimes returns a mask\n",
    "                    # smaller than the given dimensions. If so, resize it.\n",
    "                    if m.shape[0] != image_info[\"height\"] or m.shape[1] != image_info[\"width\"]:\n",
    "                        m = np.ones([image_info[\"height\"], image_info[\"width\"]], dtype=bool)\n",
    "                instance_masks.append(m)\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "        # Pack instance masks into an array\n",
    "        if class_ids:\n",
    "            mask = np.stack(instance_masks, axis=2).astype(np.bool)\n",
    "            class_ids = np.array(class_ids, dtype=np.int32)\n",
    "            return mask, class_ids\n",
    "        \n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return a link to the image in its source Website or details about\n",
    "        the image that help looking it up or debugging it.\n",
    "        Override for your dataset, but pass to this function\n",
    "        if you encounter images not in your dataset.\n",
    "        \"\"\"\n",
    "        # assume user provided the integer id of the image\n",
    "        for img in self.image_info:\n",
    "            if img['id'] == image_id:\n",
    "                return img['path']\n",
    "\n",
    "        # check if the user entered the file name\n",
    "        for img in self.image_info:\n",
    "            if img['file_name'] == image_id:\n",
    "                return img['path']\n",
    "        \n",
    "        print (\"Image '{}' not found.\".format(image_id))\n",
    "        return None\n",
    "\n",
    "    \n",
    "    def kaggle_rle_decode(self, ann, h, w):\n",
    "        \"\"\"\n",
    "        https://github.com/amirassov/kaggle-imaterialist/blob/master/src/rle.py\n",
    "        Takes uncompressed RLE for a single mask.  Returns binary mask.\n",
    "        param: ann - annotation including uncompressed rle in ['segmentation']['counts'] \n",
    "        -- where counts is a list of integers.  Also includes 'size' which is a list [int(h), int(w)] \n",
    "        \"\"\"\n",
    "        rle = ann['segmentation']['counts']\n",
    "        \n",
    "        starts, lengths = map(np.asarray, (rle[::2], rle[1::2]))\n",
    "        starts -= 1\n",
    "        ends = starts + lengths\n",
    "        img = np.zeros(h * w, dtype=np.uint8)\n",
    "        for lo, hi in zip(starts, ends):\n",
    "            img[lo:hi] = 1\n",
    "        return img.reshape((w, h)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
